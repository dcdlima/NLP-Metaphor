{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9PPNxdc4UzbsRQPQV4kMO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcdlima/NLP-Metaphor/blob/main/Metaphor_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riKblVUtoo99"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, GRU, Dense, Attention, Dropout, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset (replace 'dataset.csv' with your dataset file)\n",
        "# The dataset should have two columns: 'text' and 'label'\n",
        "data = pd.read_csv('dataset.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "texts = data['text'].values\n",
        "labels = data['label'].values\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=20000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Pad sequences\n",
        "max_length = 100\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert labels to categorical\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)  # Assuming 3 classes: metaphor, sarcasm, irony\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=3)\n",
        "\n",
        "# Define the model\n",
        "def create_model(vocab_size, embedding_dim, input_length):\n",
        "    inputs = Input(shape=(input_length,))\n",
        "    x = Embedding(vocab_size, embedding_dim, input_length=input_length)(inputs)\n",
        "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
        "    x = Attention()([x, x])  # Attention mechanism\n",
        "    x = GRU(64, return_sequences=False)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(3, activation='softmax')(x)  # 3 classes\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(word_index) + 1\n",
        "embedding_dim = 100\n",
        "\n",
        "# Create the model\n",
        "model = create_model(vocab_size, embedding_dim, max_length)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "# Generate classification report\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "print(classification_report(y_true, y_pred_classes, target_names=['Metaphor', 'Sarcasm', 'Irony']))"
      ]
    }
  ]
}